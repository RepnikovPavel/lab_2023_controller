
В основе работы лежит следущее выражение  
$$\text{ЕСЛИ распределение } \xi \text{ есть } p_{\xi | \gamma}(x|\text{номер правила})
\text{ ТО распределение } \eta \text{ есть } p_{\eta| \gamma}(y|\text{номер правила}) 
$$

Такая запись позволяет перевести утверждения модельера исследователя в совместное  
распределение наблюдений $\xi$ и действий $\eta$ RL агента.  

Релизован алгоритм восстановления совместной плотности $p_{\xi,\eta}$ по
набору "ЕСЛИ-ТО" правил.

Имея совместное распределение $p_{\xi,\eta}$, явным образом задающее ограничения, 
можно учесть эти ограничения в других алгоритмах принятия решений, чему и посвящена 
вторая часть работы.



# навигатор по файлам  

train_policy.ipynb  
построение алгоритма policy optimization    
___

create_Phi.py  
построение множества решений Phi

make_vector_representation.ipynb  
перевести решения в вектора  

descent_in_distrib_of_rules.ipynb  
оптимизация на одном решении при варьировании распр. правил    
descent_in_simplex_space.ipynb  
оптимизация на множестве решений  



___

fuzzy_operations.py  
построить график отрицания через вероятность и т.д.  
___


