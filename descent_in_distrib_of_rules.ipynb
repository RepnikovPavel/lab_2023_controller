{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import config\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from Alg.solving_algorithm import ModelGenerator\n",
    "from CustomModels.my_models import Integrator\n",
    "from CustomModels.my_models import weighted_amount,renormolize_distribution\n",
    "from aml.plotting import *\n",
    "from Losses.Losses import *\n",
    "from sklearn import decomposition\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def adjust_alpha(alpha_n):\n",
    "    for j in range(len(alpha_n)):\n",
    "        if alpha_n[j]<0.0:\n",
    "            alpha_n[j] = 0.0\n",
    "        elif alpha_n[j] > 1.0:\n",
    "            alpha_n[j] = 1.0\n",
    "    alpha_n = alpha_n/np.sum(alpha_n)\n",
    "    return alpha_n \n",
    "\n",
    "def Temp(k,T_0,max_iter):\n",
    "    # return T_0/np.log(k+2)\n",
    "    # return T_0/k\n",
    "    a = -T_0/max_iter\n",
    "    b = T_0\n",
    "    return a*k+b\n",
    "\n",
    "def ProbOfChangeToNewParams(L_new, L_old, beta, current_temp):\n",
    "    H_ = 0.0\n",
    "    if L_new < L_old:\n",
    "        H_ = 1.0\n",
    "    else:\n",
    "        H_=  np.minimum(1.0,np.exp((L_old-L_new)/(beta*current_temp)))\n",
    "        # H_ = 0.0\n",
    "    return H_\n",
    "\n",
    "def UpdateMu(mu_current,mu_new,epsilon,ProbOfUpdate):\n",
    "    if ProbOfUpdate == 1.0:\n",
    "        # new_ = epsilon*mu_current + (1-epsilon)*mu_new\n",
    "        new_ = mu_new\n",
    "        new_ = new_/np.sum(new_)\n",
    "        return  new_, True\n",
    "    if ProbOfUpdate == 0.0:\n",
    "        return  mu_current, False\n",
    "    \n",
    "    v_ = ProbOfUpdate*1000\n",
    "    coin_=  np.random.randint(0,1001)\n",
    "    if coin_ <= v_:\n",
    "        new_ = mu_new\n",
    "        # new_ = epsilon*mu_current + (1-epsilon)*mu_new\n",
    "        new_ = new_/np.sum(new_)\n",
    "        return new_, True\n",
    "    else:\n",
    "        return mu_current,False\n",
    "        \n",
    "def GenAlphaVec(mu_current, value_of_noise):\n",
    "    noise = np.random.rand(len(mu_current))\n",
    "    noise = noise/np.sum(noise)\n",
    "    new_ = (1.0-value_of_noise)*mu_current + value_of_noise* noise\n",
    "    new_ = new_/np.sum(new_)\n",
    "    return new_\n",
    "\n",
    "def adjust_alpha(alpha_n):\n",
    "    for j in range(len(alpha_n)):\n",
    "        if alpha_n[j]<0.0:\n",
    "            alpha_n[j] = 0.001\n",
    "        elif alpha_n[j] > 1.0:\n",
    "            alpha_n[j] = 1.0\n",
    "    alpha_n = alpha_n/np.sum(alpha_n)\n",
    "    return alpha_n \n",
    "\n",
    "def grad_descent_from_alpha(alpha_vec,p_0, shared_integration_supports,a,board,exp_meta,\n",
    "                            MAX_ITER,MAX_GENERATIONS,T_0):\n",
    "    d= len(alpha_vec)   \n",
    "    alpha_n = np.copy(alpha_vec)\n",
    "    alpha_n = alpha_n/np.sum(alpha_n)\n",
    "    p_n = renormolize_distribution(p_0,a,alpha_n)\n",
    "    L_n = get_L2_Distrib4D(p_n, shared_integration_supports)\n",
    "\n",
    "    for ITER in range(MAX_ITER):\n",
    "        board.Push(experiment_metadata=exp_meta,\n",
    "        x=ITER,y= L_n, label='L')\n",
    "        if ITER % 10 ==0:\n",
    "            print('ITER {} L_n {}'.format(ITER,L_n))\n",
    "            # print(alpha_n)\n",
    "        else:\n",
    "            print('ITER {} L_n {}'.format(ITER,L_n))\n",
    "\n",
    "        p_n=  renormolize_distribution(p_n,a,alpha_n)\n",
    "        L_n = get_L2_Distrib4D(p_n,shared_integration_supports)\n",
    "        gradient_ = np.zeros(shape=(d,))\n",
    "        epsilon_ = 10**(-1)\n",
    "        for j in range(d):\n",
    "            if alpha_n[j] < epsilon_:\n",
    "                alpha_1 = np.copy(alpha_n)\n",
    "                alpha_1[j] = alpha_1[j] + epsilon_\n",
    "                p_1 = renormolize_distribution(p_n,a,alpha_1)\n",
    "                L_1 = get_L2_Distrib4D(p_1,shared_integration_supports)\n",
    "                gradient_[j] = (L_1 - L_n)/epsilon_\n",
    "                continue\n",
    "            if alpha_n[j] > 1.0-epsilon_:\n",
    "                alpha_2 = np.copy(alpha_n)\n",
    "                alpha_2[j] = alpha_2[j] - epsilon_\n",
    "                p_2 = renormolize_distribution(p_n,a,alpha_2)\n",
    "                L_2 = get_L2_Distrib4D(p_2,shared_integration_supports)\n",
    "                gradient_[j] = (L_n-L_2)/epsilon_\n",
    "                continue\n",
    "\n",
    "            alpha_1 = np.copy(alpha_n)\n",
    "            alpha_1[j] = alpha_1[j] + epsilon_\n",
    "            alpha_2 = np.copy(alpha_n)\n",
    "            alpha_2[j] = alpha_2[j] - epsilon_\n",
    "\n",
    "            p_1 = renormolize_distribution(p_n,a,alpha_1)\n",
    "            p_2 = renormolize_distribution(p_n,a,alpha_2)\n",
    "\n",
    "            L_1 = get_L2_Distrib4D(p_1,shared_integration_supports)\n",
    "            L_2 = get_L2_Distrib4D(p_2,shared_integration_supports)\n",
    "            gradient_[j] = (L_1 - L_2)/(2*epsilon_)\n",
    "        board.Push(experiment_metadata=exp_meta,\n",
    "        x=ITER,y= np.linalg.norm(gradient_), label='grad norm')\n",
    "        lambda_vec = np.logspace(start=4,stop=-4,num=10)\n",
    "        lambda_best = None\n",
    "        loss_current = L_n\n",
    "        ls = []\n",
    "        lambda_ls = []\n",
    "        for lambda_ in lambda_vec:\n",
    "            alpha_copy = np.copy(alpha_n)\n",
    "            alpha_after = alpha_copy - lambda_*gradient_\n",
    "            if np.sum(alpha_after < 0.0)==d:\n",
    "                continue\n",
    "            alpha_after = adjust_alpha(alpha_after)        \n",
    "            p_after = renormolize_distribution(p_n,a,alpha_after)\n",
    "            L_after = get_L2_Distrib4D(p_after,shared_integration_supports)\n",
    "            ls.append(L_after)\n",
    "            lambda_ls.append(lambda_)\n",
    "            if L_after<loss_current:\n",
    "                loss_current = L_after\n",
    "                lambda_best = lambda_\n",
    "        arg_best = np.argsort(ls)[0]\n",
    "        left_pos = np.maximum(0, arg_best-1)\n",
    "        right_pos = np.minimum(len(ls)-1, arg_best+1)\n",
    "\n",
    "        left = lambda_ls[left_pos]\n",
    "        right = lambda_ls[right_pos]\n",
    "\n",
    "        lambda_vec = np.linspace(left,right,10)\n",
    "        addls = []\n",
    "        addlambda_ls = []\n",
    "        for lambda_ in lambda_vec:\n",
    "            alpha_copy = np.copy(alpha_n)\n",
    "            alpha_after = alpha_copy - lambda_*gradient_\n",
    "            if np.sum(alpha_after < 0.0)==d:\n",
    "                continue\n",
    "            alpha_after = adjust_alpha(alpha_after)        \n",
    "            p_after = renormolize_distribution(p_n,a,alpha_after)\n",
    "            L_after = get_L2_Distrib4D(p_after,shared_integration_supports)\n",
    "            addls.append(L_after)\n",
    "            addlambda_ls.append(lambda_)\n",
    "            if L_after<loss_current:\n",
    "                loss_current = L_after\n",
    "                lambda_best = lambda_\n",
    "        if lambda_best == None:\n",
    "            alpha_n = alpha_n + epsilon_*np.random.rand(d)\n",
    "            alpha_n = alpha_n/np.sum(alpha_n)\n",
    "        else:\n",
    "            fig,ax = plt.subplots()\n",
    "            ax.plot(lambda_ls, ls)\n",
    "            ax.set_xscale('log')\n",
    "            plt.show()\n",
    "\n",
    "            alpha_n = alpha_n - lambda_best*gradient_\n",
    "            alpha_n = adjust_alpha(alpha_n)\n",
    "        print(alpha_n)\n",
    "\n",
    "    p_n = renormolize_distribution(p_n,a,alpha_n)\n",
    "    return p_n, L_n,alpha_n\n",
    "\n",
    "    # alpha_best_ = None\n",
    "    # T_n = T_0\n",
    "    # p_n = p_0\n",
    "    # L_best = 10**99\n",
    "    # L_current = 10**99\n",
    "    # # all_losses = []\n",
    "    # # while True:\n",
    "    # #     p_n=  renormolize_distribution(p_n,a,alpha_n)\n",
    "    # #     L_n = get_L2_Distrib4D(p_n,shared_integration_supports)\n",
    "        \n",
    "    # #     n +=1    \n",
    "    # l_all = []\n",
    "    # for ITER in tqdm(range(MAX_ITER)):\n",
    "    #     sigma_n = T_n/T_0\n",
    "    #     for j in range(MAX_GENERATIONS):\n",
    "    #         alpha_ = GenAlphaVec(mu_current=alpha_n, value_of_noise=0.3*sigma_n)\n",
    "    #         p_ = renormolize_distribution(p_n, a, alpha_)\n",
    "    #         L_new = get_L2_Distrib4D(p_,shared_integration_supports)\n",
    "    #         H_ = ProbOfChangeToNewParams(L_new,L_current, 1.0,T_n)\n",
    "    #         board.Push(experiment_metadata=exp_meta,\n",
    "    #             x=ITER,y= H_, label='H')\n",
    "    #         alpha_updated, is_update = UpdateMu(alpha_n,alpha_,epsilon=0.8,ProbOfUpdate=H_)\n",
    "    #         if is_update:\n",
    "    #             board.Push(experiment_metadata=exp_meta,\n",
    "    #             x=ITER,y= L_new, label='L')\n",
    "\n",
    "    #         if is_update and (L_new<L_current):\n",
    "    #             L_current = L_new\n",
    "    #             alpha_n = alpha_updated\n",
    "    #             break\n",
    "    #         elif is_update and not (L_new<L_current):\n",
    "    #             L_current = L_new\n",
    "    #             alpha_n = alpha_updated\n",
    "\n",
    "    #         if L_new < L_best:\n",
    "    #             L_best = L_new\n",
    "    #             alpha_best_ = np.copy(alpha_updated)\n",
    "    #     if ITER % 10 ==0:\n",
    "    #         print(alpha_n)\n",
    "    #     T_n = Temp(ITER,T_0,MAX_ITER)\n",
    "    #     board.Push(experiment_metadata=exp_meta,\n",
    "    #             x=ITER,y= T_n, label='T')\n",
    "    # p_n = renormolize_distribution(p_n,a,alpha_n)\n",
    "    # return p_n, L_current,alpha_n\n",
    "\n",
    "\n",
    "def approx_equal(x1,x2,precision):\n",
    "    if np.absolute(x1-x2) < precision:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = TensorBoard(tensorboard_exe_path=config.tensorboard_path,\n",
    "                    logdir=os.path.join(config.task_dir, 'rules_descent_log'),\n",
    "                    port= '64001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = ModelGenerator(rules=config.rules,\n",
    "                            cache_dir=config.Phi_cache_dir,\n",
    "                            clear_cache=False)\n",
    "shared_integration_supports = Integrator(dir_=config.integrator_dir,\n",
    "                                        shared_data=mg.shared_data,\n",
    "                                        clear_cache=True).shared_integration_supports\n",
    "N=1000\n",
    "all_p = [torch.load(os.path.join(mg.cache_dir, 'distrib4D_{}.txt'.format(i))) for i in range(N)]\n",
    "all_v=  torch.load(os.path.join(config.task_dir, 'L2_for_Phi.txt'))\n",
    "nr = len(all_p[0].z_list)\n",
    "uniform_distrib_of_rules= np.ones(shape=(nr,))/nr\n",
    "a, h, f, coeff_list = mg.shared_data['ahfcoeff_list']\n",
    "a = [a[el].detach().numpy() for el in range(len(a))]\n",
    "for i in range(N):\n",
    "    all_p[i] = renormolize_distribution(all_p[i],a,uniform_distrib_of_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose start distribtuion by random choise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_0 = all_p[np.argmin(all_v)]\n",
    "p_0 = all_p[np.random.randint(0,N)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make gradiend descent on distribution in $R^{n_{r}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_metadata = 'gradien_descent'+get_time()\n",
    "board.InitExperiment(experiment_metadata= exp_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_n = np.copy(uniform_distrib_of_rules)\n",
    "T_0 = 0.002\n",
    "MAX_GENERATIONS = 1000\n",
    "MAX_ITER = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_best, l_best,alpha_n = grad_descent_from_alpha(alpha_n, p_0, shared_integration_supports,a,board,exp_metadata,MAX_ITER,MAX_GENERATIONS,T_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
