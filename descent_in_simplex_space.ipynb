{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import config\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from Alg.solving_algorithm import ModelGenerator\n",
    "from CustomModels.my_models import Integrator\n",
    "from CustomModels.my_models import weighted_amount,renormolize_distribution\n",
    "from aml.plotting import *\n",
    "from Losses.Losses import *\n",
    "from sklearn import decomposition\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def adjust_alpha(alpha_n):\n",
    "    for j in range(len(alpha_n)):\n",
    "        if alpha_n[j]<0.0:\n",
    "            alpha_n[j] = 0.0\n",
    "        elif alpha_n[j] > 1.0:\n",
    "            alpha_n[j] = 1.0\n",
    "    alpha_n = alpha_n/np.sum(alpha_n)\n",
    "    return alpha_n \n",
    "\n",
    "def grad_descent_from_alpha_in_simplex(alpha_vec,p_list, shared_integration_supports):\n",
    "    d = len(p_list)\n",
    "    alpha_n = np.copy(alpha_vec)\n",
    "    alpha_n = alpha_n/np.sum(alpha_n)\n",
    "    n = 0\n",
    "    p_mid = None\n",
    "    L_mid = None\n",
    "    all_losses = []\n",
    "    while True:\n",
    "        p_mid = weighted_amount(list_of_distributions=p_list, alpha_list=alpha_n)\n",
    "        L_mid = get_L2_Distrib4D(p_mid,shared_integration_supports)\n",
    "        gradient_ = np.zeros(shape=(d,))\n",
    "        alpha_not_zero = alpha_n[np.argwhere(alpha_n != 0.0)]\n",
    "        alpha_min_ = np.min(alpha_not_zero)\n",
    "        # epsilon_ = np.minimum(alpha_min_/2.0, 1.0/d)\n",
    "        epsilon_ = 10**(-3)\n",
    "        for j in range(d):\n",
    "            if alpha_n[j] < epsilon_:\n",
    "                alpha_1 = np.copy(alpha_n)\n",
    "                alpha_1[j] = alpha_1[j] + epsilon_\n",
    "                p_1 = weighted_amount(list_of_distributions=p_list, alpha_list=alpha_1)\n",
    "                L_1 = get_L2_Distrib4D(p_1,shared_integration_supports)\n",
    "                gradient_[j] = (L_1 - L_mid)/epsilon_\n",
    "                continue\n",
    "            if alpha_n[j] > 1.0-epsilon_:\n",
    "                alpha_2 = np.copy(alpha_n)\n",
    "                alpha_2[j] = alpha_2[j] - epsilon_\n",
    "                p_2 = weighted_amount(list_of_distributions=p_list, alpha_list=alpha_2)\n",
    "                L_2 = get_L2_Distrib4D(p_2,shared_integration_supports)\n",
    "                gradient_[j] = (L_mid-L_2)/epsilon_\n",
    "                continue\n",
    "\n",
    "            alpha_1 = np.copy(alpha_n)\n",
    "            alpha_1[j] = alpha_1[j] + epsilon_\n",
    "            alpha_2 = np.copy(alpha_n)\n",
    "            alpha_2[j] = alpha_2[j] - epsilon_\n",
    "\n",
    "            p_1 = weighted_amount(list_of_distributions=p_list, alpha_list=alpha_1)\n",
    "            p_2 = weighted_amount(list_of_distributions=p_list, alpha_list=alpha_2)\n",
    "\n",
    "            L_1 = get_L2_Distrib4D(p_1,shared_integration_supports)\n",
    "            L_2 = get_L2_Distrib4D(p_2,shared_integration_supports)\n",
    "            # print(L_1,L_2, epsilon_)\n",
    "            gradient_[j] = (L_1 - L_2)/(2*epsilon_)\n",
    "        \n",
    "\n",
    "        lambda_vec = np.logspace(start=4,stop=-4,num=10)\n",
    "        lambda_best = None\n",
    "        loss_current = L_mid\n",
    "        ls = []\n",
    "        lambda_ls = []\n",
    "        for lambda_ in lambda_vec:\n",
    "            alpha_copy = np.copy(alpha_n)\n",
    "            alpha_after = alpha_copy - lambda_*gradient_\n",
    "            if np.sum(alpha_after < 0.0)==d:\n",
    "                continue\n",
    "            alpha_after = adjust_alpha(alpha_after)        \n",
    "            p_after = weighted_amount(list_of_distributions=p_list, alpha_list=alpha_after)\n",
    "            L_after = get_L2_Distrib4D(p_after,shared_integration_supports)\n",
    "            ls.append(L_after)\n",
    "            lambda_ls.append(lambda_)\n",
    "            if L_after<loss_current:\n",
    "                loss_current = L_after\n",
    "                lambda_best = lambda_\n",
    "        # arg_best = np.argsort(ls)[0]\n",
    "        # left_pos = np.maximum(0, arg_best-1)\n",
    "        # right_pos = np.minimum(len(ls)-1, arg_best+1)\n",
    "\n",
    "        # left = lambda_ls[left_pos]\n",
    "        # right = lambda_ls[right_pos]\n",
    "\n",
    "        # lambda_vec = np.linspace(left,right,10)\n",
    "        # addls = []\n",
    "        # addlambda_ls = []\n",
    "        # for lambda_ in lambda_vec:\n",
    "        #     alpha_copy = np.copy(alpha_n)\n",
    "        #     alpha_after = alpha_copy - lambda_*gradient_\n",
    "        #     if np.sum(alpha_after < 0.0)==d:\n",
    "        #         continue\n",
    "        #     alpha_after = adjust_alpha(alpha_after)        \n",
    "        #     p_after = weighted_amount(list_of_distributions=p_list, alpha_list=alpha_after)\n",
    "        #     L_after = get_L2_Distrib4D(p_after,shared_integration_supports)\n",
    "        #     addls.append(L_after)\n",
    "        #     addlambda_ls.append(lambda_)\n",
    "        #     if L_after<loss_current:\n",
    "        #         loss_current = L_after\n",
    "        #         lambda_best = lambda_\n",
    "        # fig,ax = plt.subplots()\n",
    "        # ax.plot(lambda_ls,ls,color= 'k')\n",
    "        # ax.plot(addlambda_ls,addls,color= 'r')\n",
    "        # plt.show()\n",
    "        \n",
    "\n",
    "        all_losses.append(loss_current)\n",
    "        if loss_current == L_mid:\n",
    "            break        \n",
    "\n",
    "        alpha_n = alpha_n - lambda_best*gradient_\n",
    "        alpha_n = adjust_alpha(alpha_n)        \n",
    "\n",
    "        # board_reference.Push(experiment_metadata=experiment_metadata,\n",
    "        #     x=n,y= lambda_best, label='best_lambda')\n",
    "        \n",
    "        # board_reference.Push(experiment_metadata=experiment_metadata,\n",
    "        #     x=n,y= np.linalg.norm(gradient_), label='grad_norm')\n",
    "\n",
    "        n+=1\n",
    "    \n",
    "    return p_mid, L_mid\n",
    "\n",
    "def approx_equal(x1,x2,precision):\n",
    "    if np.absolute(x1-x2) < precision:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def search_min_in_simplex_with_center(L_0, mid_of_simplex, simplex, p_list, shared_integration_supports):\n",
    "    pos_of_mid = np.argwhere(simplex==mid_of_simplex).flatten()[0]\n",
    "    d = len(p_list)\n",
    "    all_losses = []\n",
    "    # eps_vec = np.concatenate([np.linspace(0, 1.0, 10),np.logspace(-1,-6,10)])\n",
    "    eps_vec = np.linspace(0.1, 0.9, 5)\n",
    "    p_n = None\n",
    "    L_n = 10**99\n",
    "    loss_n = []\n",
    "    n_ = []\n",
    "    for ITER in range(10):\n",
    "        min_L_in_iter = 10**99\n",
    "        if ITER==0:\n",
    "            for i in range(len(simplex)):\n",
    "                if mid_of_simplex == simplex[i]:\n",
    "                    continue\n",
    "                for j in range(len(eps_vec)):\n",
    "                    eps_= eps_vec[j]\n",
    "                    p_j = weighted_amount(list_of_distributions=[p_list[pos_of_mid], p_list[i]], alpha_list=[1.0-eps_, eps_])\n",
    "                    L_j = get_L2_Distrib4D(p_j, shared_integration_supports)\n",
    "                    if (L_j < min_L_in_iter):\n",
    "                        min_L_in_iter = L_j\n",
    "                    if (L_j < L_n) or approx_equal(L_j, L_n, 10**(-4)):\n",
    "                        L_n = L_j\n",
    "                        p_n = p_j\n",
    "        else:\n",
    "            for i in range(len(simplex)):\n",
    "                for j in range(len(eps_vec)):\n",
    "                    eps_= eps_vec[j]\n",
    "                    p_j = weighted_amount(list_of_distributions=[p_n, p_list[i]], alpha_list=[1.0-eps_, eps_])\n",
    "                    L_j = get_L2_Distrib4D(p_j, shared_integration_supports)\n",
    "                    if (L_j < min_L_in_iter):\n",
    "                        min_L_in_iter = L_j\n",
    "                    if (L_j < L_n) or approx_equal(L_j, L_n, 10**(-4)):\n",
    "                        L_n = L_j\n",
    "                        p_n = p_j\n",
    "        if approx_equal(L_n, L_0,10**(-4)):\n",
    "            break \n",
    "        print('ITER {} LOSS {}'.format(ITER, L_n))\n",
    "        loss_n.append(L_n)\n",
    "        n_.append(ITER)\n",
    "        if min_L_in_iter >= L_0:\n",
    "            break\n",
    "\n",
    "    if L_n > L_0:\n",
    "        return p_list[pos_of_mid], L_0\n",
    "    else:\n",
    "        return p_n, L_n\n",
    "\n",
    "def make_obj_func(p_list_in_simplex, shared_integration_supports):\n",
    "    def get_l_by_alpha(alpha_vec):\n",
    "        p_ = weighted_amount(list_of_distributions=p_list_in_simplex, alpha_list=alpha_vec)\n",
    "        L_ = get_L2_Distrib4D(p_, shared_integration_supports)\n",
    "        return L_\n",
    "    return get_l_by_alpha\n",
    "\n",
    "def grad_descent(L_0,p_list,mid_of_siplex,simplex, shared_integration_supports):\n",
    "    # p_n, L_n = search_min_in_simplex_with_center(L_0,mid_of_siplex, simplex,p_list, shared_integration_supports) \n",
    "    # return p_n, L_n\n",
    "    # alpha_vec = np.zeros(shape=(len(p_list),))\n",
    "    # pos_of_mid = np.argwhere(simplex==mid_of_siplex).flatten()[0]\n",
    "    # alpha_vec[pos_of_mid] = 1.0\n",
    "    # p_mid, L_mid = grad_descent_from_alpha_in_simplex(alpha_vec,p_list, shared_integration_supports)\n",
    "    # return p_mid, L_mid\n",
    "    \n",
    "    # fmincon\n",
    "\n",
    "    d = len(p_list)\n",
    "    print('dimension of problem {}'.format(d))\n",
    "    alpha_0 = np.zeros(shape=(len(p_list),))\n",
    "    pos_of_mid = np.argwhere(simplex==mid_of_siplex).flatten()[0]\n",
    "    alpha_0[pos_of_mid] = 1.0\n",
    "    obj_func = make_obj_func(p_list, shared_integration_supports)\n",
    "    start_Loss = obj_func(alpha_0)\n",
    "    print('start from {}'.format(start_Loss))\n",
    "    # start_ = time.time()\n",
    "    # for i in range(10):\n",
    "    #     result_ = obj_func(alpha_0)\n",
    "    # delta_ = time.time()-start_\n",
    "\n",
    "    # print('time per iter {}'.format(delta_/10.0))\n",
    "    # raise SystemExit\n",
    "    bounds = np.zeros(shape=(d,2))\n",
    "    for j in range(d):\n",
    "        bounds[j][0] = 0.0\n",
    "        bounds[j][1] = 1.0\n",
    "    cons = [{\"type\": \"eq\", \"fun\": lambda x: np.sum(x)-1.0}]\n",
    "\n",
    "    options = {\n",
    "        'verbose':2\n",
    "    }\n",
    "\n",
    "    results = minimize(fun=obj_func, x0=alpha_0,method='trust-constr', bounds=bounds,constraints=cons,options=options)\n",
    "    best_alpha = results['x']\n",
    "    print(results)\n",
    "    print(best_alpha)\n",
    "    print('init vertex {}'.format(pos_of_mid))\n",
    "    fig0,ax0= plt.subplots()\n",
    "    fig0.set_size_inches(16,9)\n",
    "    ax0.bar(x= np.arange(0,d),height=best_alpha)\n",
    "    ax0.set_title(r'$P_{\\gamma}(k)$')\n",
    "    # ax0.set_xlim(-1,50)\n",
    "    ax0.set_xlabel(r'$k$')\n",
    "    plt.show()\n",
    "    raise SystemExit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = TensorBoard(tensorboard_exe_path=config.tensorboard_path,\n",
    "                    logdir=os.path.join(config.task_dir, 'descent_log'),\n",
    "                    port= '64001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = ModelGenerator(rules=config.rules,\n",
    "                            cache_dir=config.Phi_cache_dir,\n",
    "                            clear_cache=False)\n",
    "N=1000\n",
    "all_p = [torch.load(os.path.join(mg.cache_dir, 'distrib4D_{}.txt'.format(i))) for i in range(N)]\n",
    "nr = len(all_p[0].z_list)\n",
    "uniform_distrib_of_rules= np.ones(shape=(nr,))/nr\n",
    "a, h, f, coeff_list = mg.shared_data['ahfcoeff_list']\n",
    "for i in range(N):\n",
    "    all_p[i] = renormolize_distribution(all_p[i],[a[el].detach().numpy() for el in range(len(a))],uniform_distrib_of_rules)\n",
    "\n",
    "\n",
    "vectors = torch.load(config.Phi_vector_representation)\n",
    "all_v = torch.load(os.path.join(config.task_dir, 'L2_for_Phi.txt'))\n",
    "# plot_float_distribution(all_v)\n",
    "# raise SystemExit\n",
    "# print(np.sort(all_v))\n",
    "simplices = torch.load(os.path.join(config.task_dir, 'triangulation_simplexes.txt'))\n",
    "sorted_vertesices = [el for el in np.argsort(all_v)]  \n",
    "# get simliexes with best loss \n",
    "# sorted_simplixes = []\n",
    "# simplexes_best = []\n",
    "simpexes_for_omptimization = []\n",
    "for best_vertex in sorted_vertesices:\n",
    "    vertex_siplixes = []\n",
    "    for simplex in simplices:\n",
    "        if best_vertex in  simplex:\n",
    "            vertex_siplixes.append(simplex)\n",
    "    unique_vertexes = np.unique(vertex_siplixes)\n",
    "    simpexes_for_omptimization.append(unique_vertexes)\n",
    "\n",
    "print('number of simplexes for oprimization {}'.format(len(simpexes_for_omptimization)))\n",
    "print('len of simplex {}'.format(len(simpexes_for_omptimization[0])))\n",
    "\n",
    "d = len(vectors[0])\n",
    "support_vertexes = torch.load(os.path.join(config.task_dir, 'support_points.txt'))\n",
    "\n",
    "mg = ModelGenerator(rules=config.rules,\n",
    "                            cache_dir=config.Phi_cache_dir,\n",
    "                            clear_cache=False)\n",
    "shared_integration_supports = Integrator(dir_=config.integrator_dir,\n",
    "                                        shared_data=mg.shared_data,\n",
    "                                        clear_cache=True).shared_integration_supports\n",
    "\n",
    "\n",
    "\n",
    "# board = TensorBoard(tensorboard_exe_path=config.tensorboard_path,\n",
    "#                     logdir=os.path.join(config.task_dir, 'descent_log'),\n",
    "#                     port= '64001')\n",
    "# exp_metadata = 'sorted_simplexes'+get_time()\n",
    "# board.InitExperiment(experiment_metadata= exp_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start best {}'.format(np.min(all_v)))\n",
    "global_L_min = 10**99\n",
    "p_best = None\n",
    "# losses_per_simplex_optimization = {i:[] for i in range(len(sorted_simplixes))}\n",
    "# max_iter = np.maximum(int(0.005*len(sorted_simplixes)),1)\n",
    "max_iter = N\n",
    "for i in tqdm(range(max_iter)):\n",
    "    # j = np.random.randint(0, len(sorted_simplixes))\n",
    "    # simplex = sorted_simplixes[j]\n",
    "    # BestOfSimplex = simplexes_best[j]\n",
    "    simplex = simpexes_for_omptimization[i]\n",
    "\n",
    "    p_in_simplex = [all_p[el] for el in simplex]\n",
    "    mid_of_siplex = sorted_vertesices[i]\n",
    "    BestOfSimplex = all_v[mid_of_siplex]\n",
    "\n",
    "    # print(BestOfSimplex)\n",
    "    L_0 = BestOfSimplex\n",
    "    p_best, L_best = grad_descent(L_0,p_in_simplex,mid_of_siplex,simplex, shared_integration_supports)\n",
    "    if L_best < global_L_min:\n",
    "        global_L_min = L_best\n",
    "        p_best = p_best\n",
    "        torch.save(p_best, config.Phi_descent_best_p_path)\n",
    "    print('simplex L_0 {} simplex fitted L {}'.format(BestOfSimplex, L_best))\n",
    "\n",
    "print(global_L_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
